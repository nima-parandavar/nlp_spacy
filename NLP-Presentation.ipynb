{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5e9bce2",
   "metadata": {},
   "source": [
    "# NLP with spaCy in python\n",
    "\n",
    "### Nima Parandavar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb5e8f3",
   "metadata": {},
   "source": [
    "   # How NLP works ?\n",
    "   computers are emotionless, how it possible to train them to understand human language ?\n",
    "   We need a system that can translate our language to numbers.\n",
    "   \n",
    "   ### Look at that examples\n",
    "   - the 0.0897 0.0160 -0.0571 0.0405 -0.0696  ...\n",
    "   - and -0.0314 0.0149 -0.0205 0.0557 0.0205  ...\n",
    "   - of -0.0063 -0.0253 -0.0338 0.0178 -0.0966 ...\n",
    "   - to 0.0495 0.0411 0.0041 0.0309 -0.0044    ...\n",
    "   - in -0.0234 -0.0268 -0.0838 0.0386 -0.0321 ...\n",
    "   \n",
    "   If we visual these word we can see the words \"the, and, of, to, in \" are so close togather.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29acd76f",
   "metadata": {},
   "source": [
    "# How machine learning helps us to use NLP?\n",
    "Machine learning allows you to accomplish three tasks: syntactic dependency parsing (determining the relationships between words in a sentence), part-of-speech tagging (identifying nouns, verbs, and other parts of speech), and named entity recognition (sorting proper nouns into categories like people, organizations, and locations)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cef7bb4",
   "metadata": {},
   "source": [
    "# Working with Spacy\n",
    "#### Spacy is a open source library, you can use it to create varety applications with that. support 74+ languages\n",
    "#### It use machine learning to proccess texts\n",
    "\n",
    "![alt text for screen reader](./img1.png)\n",
    "\n",
    "#### Spacy use Nural network to train and predict word and grammers\n",
    "\n",
    "![alt text for screen reader](./img2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f92377",
   "metadata": {},
   "source": [
    "# What can spacy do ?\n",
    "#### SpaCy uses neural models for syntactic dependency parsing, part-of-speech tagging, and named entity recognition.\n",
    "# What can not spacy do?\n",
    "#### One thing spaCy can’t do for you is recognized the user’s intent.\n",
    "## Look at the following example:\n",
    "#### I want to order a pair of jeans\n",
    "![alt text for screen reader](./img3.png)\n",
    "#### Notice that spaCy doesn’t mark anything as the user’s intent in the generated tree. In fact, it would be strange if it did so. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9ebadc",
   "metadata": {},
   "source": [
    "# THE TEXT-PROCESSING PIPELINE\n",
    "\n",
    "### Setup spaCy\n",
    "\n",
    "### Follow the commands to install scpCy\n",
    "- pip install -U spacy\n",
    "- python -m spacy info\n",
    "### Install Statical Model for spaCy\n",
    "#### SpaCy dosen't have any models, we have to install this models that it understand\n",
    "- python -m spacy download en_core_web_sm\n",
    "### Now let's start :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11e22d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54a7fce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are many Model for spacy\n",
    "# language_core_web_sm / md / ls (use web to collect words)\n",
    "# language_core_wiki_sm / ms / ls (use wikipedia to collect words)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f06bad6",
   "metadata": {},
   "source": [
    "# Basic NLP operation with spaCy\n",
    "![alt text for screen reader](./img4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c72689",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "#### The very first action any NLP application typically performs on a text is parsing that text into tokens\n",
    "#### Tokenization is the first operation because all the other operations require you to have tokens already in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43d1cb73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "'m\n",
      "flying\n",
      "to\n",
      "London\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "string = u\"I'm flying to London.\"\n",
    "doc = nlp(string)\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75f3b005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'have', 'presentation', 'about', 'NLP', '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = u\"I have presentation about NLP.\"\n",
    "doc = nlp(string)\n",
    "[token.text for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f5a591",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "A lemma is the base form of a token. You can think of it as the form in which the token would appear if it were listed in a dictionary. For example, the lemma for the token “flying” is “fly.” Lemmatization is the process of reducing word forms to their lemma. The following script provides a simple example of how to do lemmatization with spaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "599be1fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this \t\t this\n",
      "product \t\t product\n",
      "integrates \t\t integrate\n",
      "both \t\t both\n",
      "libraries \t\t library\n",
      "for \t\t for\n",
      "downloading \t\t download\n",
      "and \t\t and\n",
      "applying \t\t apply\n",
      "patches \t\t patch\n"
     ]
    }
   ],
   "source": [
    "string = u\"this product integrates both libraries for downloading and applying patches\"\n",
    "doc = nlp(string)\n",
    "for token in doc:\n",
    "    print(token.text, \"\\t\\t\", token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52e40a6",
   "metadata": {},
   "source": [
    "# part-of-speech\n",
    "#### A part-of-speech tag tells you the part-of-speech (noun, verb, and so on) of a given word in a given sentence.\n",
    "#### part-of-speech tags can include detailed information about a token. In the case of verbs, they might tell you the following features: tense (past, present, or future), aspect (simple, progressive, or perfect), person (1st, 2nd, or 3rd), and number (singular or plural)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98eabfd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \t PRON\n",
      "have \t AUX\n",
      "flown \t VERB\n",
      "to \t ADP\n",
      "LA \t PROPN\n",
      ", \t PUNCT\n",
      "now \t ADV\n",
      "flying \t VERB\n",
      "to \t ADP\n",
      "London \t PROPN\n",
      ". \t PUNCT\n"
     ]
    }
   ],
   "source": [
    "string = \"I have flown to LA, now flying to London.\"\n",
    "doc = nlp(string)\n",
    "for token in doc:\n",
    "    print(token.text, \"\\t\", token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0291956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \t PRP\n",
      "have \t VBP\n",
      "flown \t VBN\n",
      "to \t IN\n",
      "LA \t NNP\n",
      ", \t ,\n",
      "now \t RB\n",
      "flying \t VBG\n",
      "to \t IN\n",
      "London \t NNP\n",
      ". \t .\n"
     ]
    }
   ],
   "source": [
    "string = \"I have flown to LA, now flying to London.\"\n",
    "doc = nlp(string)\n",
    "for token in doc:\n",
    "    print(token.text, \"\\t\", token.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed26e5ee",
   "metadata": {},
   "source": [
    "![alt text for screen reader](./img5.png)\n",
    "\n",
    "# So, how can part-of-speech help us ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fda77b4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['flying']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = u'I have flown to LA. Now I am flying to Frisco.'\n",
    "doc = nlp(string)\n",
    "[token.text for token in doc if token.tag_ == \"VBG\" or token.tag_ == \"VB\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d0c201",
   "metadata": {},
   "source": [
    "# Syntatic relations\n",
    "Now let’s combine the proper nouns with the verb that the part-of-speech tagger selected earlier. Recall that the list of verbs you could potentially use to identify the intent of the discourse contains only the verb “flying” in the second sentence. How can you get the verb/proper noun pair that best describes the intent behind the discourse? A human would obviously compose the verb/proper noun pairs from words found in the same sentence. Because the verb “flown” in the first sentence doesn’t meet the condition specified (remember that only infinitive and present progressive forms meet the condition), you’d be able to compose such a pair for the second sentence only: “flying, Frisco.”\n",
    "\n",
    "![alt text for screen reader](./img6.png)\n",
    "# Here some common dependecy labels\n",
    "![alt text for screen reader](./img7.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6fc0d26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \t PRON \t nsubj\n",
      "have \t AUX \t aux\n",
      "flown \t VERB \t ROOT\n",
      "to \t ADP \t prep\n",
      "LA \t PROPN \t pobj\n",
      ". \t PUNCT \t punct\n",
      "Now \t ADV \t advmod\n",
      "I \t PRON \t nsubj\n",
      "am \t AUX \t aux\n",
      "flying \t VERB \t ROOT\n",
      "to \t ADP \t prep\n",
      "Frisco \t PROPN \t pobj\n",
      ". \t PUNCT \t punct\n"
     ]
    }
   ],
   "source": [
    "string = \"I have flown to LA. Now I am flying to Frisco.\"\n",
    "doc = nlp(string)\n",
    "for token in doc:\n",
    "    print(token.text, \"\\t\", token.pos_, \"\\t\", token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09818cb6",
   "metadata": {},
   "source": [
    "#### But what it doesn’t show you is how words are related to each other in a sentence by means of the commonly called dependency arcs explained at the beginning of this section. To look at the dependency arcs in the sample discourse, replace the loop in the preceding script with the following one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b07f4a71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \t flown \t PRON \t nsubj\n",
      "have \t flown \t AUX \t aux\n",
      "flown \t flown \t VERB \t ROOT\n",
      "to \t flown \t ADP \t prep\n",
      "LA \t to \t PROPN \t pobj\n",
      ". \t flown \t PUNCT \t punct\n",
      "Now \t flying \t ADV \t advmod\n",
      "I \t flying \t PRON \t nsubj\n",
      "am \t flying \t AUX \t aux\n",
      "flying \t flying \t VERB \t ROOT\n",
      "to \t flying \t ADP \t prep\n",
      "Frisco \t to \t PROPN \t pobj\n",
      ". \t flying \t PUNCT \t punct\n"
     ]
    }
   ],
   "source": [
    "string = \"I have flown to LA. Now I am flying to Frisco.\"\n",
    "doc = nlp(string)\n",
    "for token in doc:\n",
    "    print(token.text, \"\\t\", token.head.text  , \"\\t\", token.pos_, \"\\t\", token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b7d56248",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \t flown \t PRON \t nsubj\n",
      "have \t flown \t AUX \t aux\n",
      "flown \t flying \t VERB \t ccomp\n",
      "to \t flown \t ADP \t prep\n",
      "LA \t to \t PROPN \t pobj\n",
      ", \t flying \t PUNCT \t punct\n",
      "Now \t flying \t ADV \t advmod\n",
      "I \t flying \t PRON \t nsubj\n",
      "am \t flying \t AUX \t aux\n",
      "flying \t flying \t VERB \t ROOT\n",
      "to \t flying \t ADP \t prep\n",
      "Frisco \t to \t PROPN \t pobj\n",
      ". \t flying \t PUNCT \t punct\n"
     ]
    }
   ],
   "source": [
    "string = \"I have flown to LA, Now I am flying to Frisco.\"\n",
    "doc = nlp(string)\n",
    "for token in doc:\n",
    "    print(token.text, \"\\t\", token.head.text  , \"\\t\", token.pos_, \"\\t\", token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c904abf5",
   "metadata": {},
   "source": [
    "## Example: return Roots and object of preposition of sentences\n",
    "\n",
    "##### You can travers through sentenses with your_nlp_object.sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "291c436e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['flown', 'LA']\n",
      "['flying', 'Frisco']\n"
     ]
    }
   ],
   "source": [
    "string = u'I have flown to LA. Now I am flying to Frisco.'\n",
    "doc = nlp(string)\n",
    "for sent in doc.sents:\n",
    "    print([token.text for token in sent if token.dep_ == \"ROOT\" or token.dep_ == \"pobj\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cf4de2",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9b7b3417",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \t 0\n",
      "have \t 0\n",
      "flown \t 0\n",
      "to \t 0\n",
      "LA \t 384\n",
      ". \t 0\n",
      "Now \t 0\n",
      "I \t 0\n",
      "am \t 0\n",
      "flying \t 0\n",
      "to \t 0\n",
      "Frisco \t 383\n",
      ". \t 0\n"
     ]
    }
   ],
   "source": [
    "string = u'I have flown to LA. Now I am flying to Frisco.'\n",
    "doc = nlp(string)\n",
    "for token in doc:\n",
    "    print(token.text, \"\\t\", token.ent_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc07fcd4",
   "metadata": {},
   "source": [
    "# You can also see what kind of entity the word is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0e00516d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \t \n",
      "have \t \n",
      "flown \t \n",
      "to \t \n",
      "LA \t GPE\n",
      ". \t \n",
      "Now \t \n",
      "I \t \n",
      "am \t \n",
      "flying \t \n",
      "to \t \n",
      "London \t GPE\n",
      ". \t \n"
     ]
    }
   ],
   "source": [
    "string = u'I have flown to LA. Now I am flying to London.'\n",
    "doc = nlp(string)\n",
    "for token in doc:\n",
    "    print(token.text, \"\\t\", token.ent_type_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467afa7e",
   "metadata": {},
   "source": [
    "# GPE is geopolitical entity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6e75ba",
   "metadata": {},
   "source": [
    "# Find left childs of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f9eefdc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5  - counting the words\n",
      "[] \t\t I\n",
      "[I] \t\t want\n",
      "[] \t\t a\n",
      "[] \t\t green\n",
      "[a, green] \t\t apple\n"
     ]
    }
   ],
   "source": [
    "string = u\"I want a green apple\"\n",
    "doc = nlp(string)\n",
    "print(len(doc), \" - counting the words\")\n",
    "for token in doc:\n",
    "    print(list(token.lefts), \"\\t\\t\", token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b6b215a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[a, green]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you want just a word left childs ...\n",
    "[w for w in doc[4].lefts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "21cd6d59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A \t []\n",
      "severe \t []\n",
      "storm \t [A, severe]\n",
      "hit \t [storm, beach, .]\n",
      "the \t []\n",
      "beach \t [the]\n",
      ". \t []\n",
      "It \t []\n",
      "started \t [It, rain, .]\n",
      "to \t []\n",
      "rain \t [to]\n",
      ". \t []\n"
     ]
    }
   ],
   "source": [
    "string = \"A severe storm hit the beach. It started to rain.\"\n",
    "doc = nlp(string)\n",
    "for i in range(len(doc)):\n",
    "    \n",
    "    print(doc[i].text, \"\\t\", list(doc[i].children))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327e57eb",
   "metadata": {},
   "source": [
    "# Also you can use enumerate(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "76830f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence 0 have 1 VERB\n",
      "sentence 1 have 3 VERB\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for i, sent in enumerate(doc.sents):\n",
    "    for token in sent:\n",
    "        if token.pos_ == \"VERB\":\n",
    "            counter += 1\n",
    "    print(f\"sentence {i} have {counter} VERB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3203c7",
   "metadata": {},
   "source": [
    "# token.i to access of doc index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9c9d34b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A \t 0\n",
      "severe \t 1\n",
      "storm \t 2\n",
      "hit \t 3\n",
      "the \t 4\n",
      "beach \t 5\n",
      ". \t 6\n",
      "It \t 7\n",
      "started \t 8\n",
      "to \t 9\n",
      "rain \t 10\n",
      ". \t 11\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, \"\\t\", token.i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248bdbf6",
   "metadata": {},
   "source": [
    "# EXTRACTING AND USING LINGUISTIC FEATURES\n",
    "### with Part-of-Speech Tags\n",
    "### Suppose we want to extract price of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b9bd2966",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The \t DET \t determiner\n",
      "firm \t NOUN \t noun\n",
      "earned \t VERB \t verb\n",
      "$ \t SYM \t symbol\n",
      "1.5 \t NUM \t numeral\n",
      "million \t NUM \t numeral\n",
      "in \t ADP \t adposition\n",
      "2017 \t NUM \t numeral\n",
      ". \t PUNCT \t punctuation\n"
     ]
    }
   ],
   "source": [
    "string = \"The firm earned $1.5 million in 2017.\"\n",
    "#  let’s extract the coarse-grained part-of-speech features from the tokens\n",
    "doc = nlp(string)\n",
    "for token in doc:\n",
    "    print(token.text, \"\\t\", token.pos_, \"\\t\",  spacy.explain(token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bf4aa7f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The \t DT \t determiner\n",
      "firm \t NN \t noun, singular or mass\n",
      "earned \t VBD \t verb, past tense\n",
      "$ \t $ \t symbol, currency\n",
      "1.5 \t CD \t cardinal number\n",
      "million \t CD \t cardinal number\n",
      "in \t IN \t conjunction, subordinating or preposition\n",
      "2017 \t CD \t cardinal number\n",
      ". \t . \t punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "# fine-grained part-of-speech tags \n",
    "for token in doc:\n",
    "    print(token.text, \"\\t\", token.tag_, \"\\t\",  spacy.explain(token.tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0835ba45",
   "metadata": {},
   "source": [
    "# What diffrent between fine grained and coarse grained ?\n",
    "### Now let's extracting description of money"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e17a6f91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ 1.5 million \n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"The firm earned $1.5 million in 2017\")\n",
    "pharse = \"\"\n",
    "for token in doc:\n",
    "    if token.tag_ == \"$\":\n",
    "        pharse = token.text + \" \"\n",
    "        i = token.i + 1\n",
    "        while doc[i].tag_ == \"CD\":\n",
    "            pharse += doc[i].text + \" \"\n",
    "            i += 1\n",
    "print(pharse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7679165",
   "metadata": {},
   "source": [
    "# Turning statement into question\n",
    "Suppose your NLP application must be able to generate a question from a submitted statement. For example, one way chatbots maintain conversations with the user is by asking the user a confirmatory question. When a user says, “I am sure,” the chatbot might ask something like, “Are you really sure?” To do this, the chatbot must be able to generate a relevant question.\n",
    "\n",
    "Let’s say the user’s submitted sentence is this:\n",
    "\n",
    "#### I can promise it is worth your time.\n",
    "\n",
    "#### Give me seggest how we can create \"Can you really promise it is worth my time?\"  ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f95f398",
   "metadata": {},
   "source": [
    "# First, let's see part-of-speech labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3f473215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \t PRON \t PRP \t pronoun, personal\n",
      "can \t AUX \t MD \t verb, modal auxiliary\n",
      "promise \t VERB \t VB \t verb, base form\n",
      "it \t PRON \t PRP \t pronoun, personal\n",
      "is \t AUX \t VBZ \t verb, 3rd person singular present\n",
      "worth \t ADJ \t JJ \t adjective (English), other noun-modifier (Chinese)\n",
      "your \t PRON \t PRP$ \t pronoun, possessive\n",
      "time \t NOUN \t NN \t noun, singular or mass\n",
      ". \t PUNCT \t . \t punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"I can promise it is worth your time.\")\n",
    "for token in doc:\n",
    "    print(token.text, \"\\t\", token.pos_, \"\\t\", token.tag_,  \"\\t\",  spacy.explain(token.tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a70fed0",
   "metadata": {},
   "source": [
    "# Second, we should replace \"I\" to \"you\" becuse a chat bot wants to ask a question\n",
    "In other words, a chatbot refers to itself as “I” or “me,” and it refers to a user as “you.”\n",
    "\n",
    "The following steps outline what we need to do to generate a question from the original statement:\n",
    "\n",
    "   - Change the order of words in the original sentence from “subject + modal auxiliary verb + infinitive verb” to “modal auxiliary verb + subject + infinitive verb.”\n",
    "   - Replace the personal pronoun “I” (the sentence’s subject) with “you.”\n",
    "   - Replace the possessive pronoun “your” with “my.”\n",
    "   - Place the adverbial modifier “really” before the verb “promise” to emphasize the latter.\n",
    "   - Replace the punctuation mark “.” with “?” at the end of the sentence.\n",
    "\n",
    "![](./img8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1586dde4",
   "metadata": {},
   "source": [
    "# Third, let's dive into code ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "70dfec62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "can I promise it is worth your time."
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(u\"I can promise it is worth your time.\")\n",
    "sent = ''\n",
    "for index, token in enumerate(doc):\n",
    "    if token.tag_ == \"PRP\" and doc[index + 1].tag_ == \"MD\" and doc[index + 2].tag_ == \"VB\":\n",
    "        sent = f\"{doc[index + 1].text} {doc[index].text}\"\n",
    "        \n",
    "        sent = f\"{sent} {doc[index + 2:].text}\"\n",
    "        break\n",
    "\n",
    "doc = nlp(sent) \n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d1f592f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "can you promise it is worth your time."
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i,token in enumerate(doc):\n",
    "    if token.tag_ == 'PRP' and token.text == 'I':\n",
    "        sent = doc[:i].text + ' you ' + doc[i+1:].text\n",
    "        break\n",
    "\n",
    "doc = nlp(sent)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "092b5f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "can you promise it is worth my time."
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i,token in enumerate(doc):\n",
    "    if token.tag_ == 'PRP$' and token.text == 'your':\n",
    "       sent = doc[:i].text + ' my ' + doc[i+1:].text\n",
    "       break\n",
    "\n",
    "doc = nlp(sent)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "07fdf701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "can you really promise it is worth my time."
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i,token in enumerate(doc):\n",
    "    if token.tag_ == 'VB':\n",
    "        sent = doc[:i].text + ' really ' + doc[i:].text\n",
    "        break\n",
    "doc = nlp(sent)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "781ff65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can you really promise it is worth my time?\n"
     ]
    }
   ],
   "source": [
    "sent = doc[:len(doc)-1].text + '?'\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e086c986",
   "metadata": {},
   "source": [
    "# It is not all thing :(\n",
    "part-of-speech tags are a powerful tool for smart text processing. But in practice, you might need to know more about a sentence’s tokens to process it intelligently.\n",
    "\n",
    "For example, you might need to know whether a personal pronoun is the subject of a sentence or a grammatical object. Sometimes, this task is easy. The personal pronouns “I,” “he,” “she,” “they,” and “we” will almost always be the subject. When used as an object, “I” turns into “me,” as in “A postman brought me a letter.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02a65af",
   "metadata": {},
   "source": [
    "# Look at this example\n",
    "#### \"I know you. You know me.\"\n",
    "we can not use part-of-speech to make question, we have to use dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "38054f1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \t PRON \t PRP \t nsubj \t nominal subject\n",
      "can \t AUX \t MD \t aux \t auxiliary\n",
      "promise \t VERB \t VB \t ROOT \t root\n",
      "it \t PRON \t PRP \t nsubj \t nominal subject\n",
      "is \t AUX \t VBZ \t ccomp \t clausal complement\n",
      "worth \t ADJ \t JJ \t acomp \t adjectival complement\n",
      "your \t PRON \t PRP$ \t poss \t possession modifier\n",
      "time \t NOUN \t NN \t npadvmod \t noun phrase as adverbial modifier\n",
      ". \t PUNCT \t . \t punct \t punctuation\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'I can promise it is worth your time.')\n",
    "for token in doc:\n",
    "    print(token.text, \"\\t\", token.pos_, \"\\t\", token.tag_, \"\\t\", token.dep_, \"\\t\", spacy.explain(token.dep_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae119edd",
   "metadata": {},
   "source": [
    "#### Combining part-of-speech tags and dependency labels can give you a better picture of the grammatical role of each token in a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fadfe7",
   "metadata": {},
   "source": [
    "# Similarity words\n",
    "\n",
    "#### Look at the example\n",
    "![](./img9.png)\n",
    "![](./img10.png)\n",
    "##### spaCy’s small models (those whose model size indicator is %sm) don’t include word vectors. You can still use the similarity method with these models to compare tokens, spans, and documents, but the results won’t be as accurate.\n",
    "\n",
    "# Let's see how can find similarity words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5cdc8aec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nima\\AppData\\Local\\Temp\\ipykernel_7420\\1155180742.py:2: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  doc.similarity(doc[2:5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5759161577864177"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"I want a green apple.\")\n",
    "doc.similarity(doc[2:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a615b172",
   "metadata": {},
   "source": [
    "#### The warning says we use a small model and the accuracy of similarity words are is not good ( it is not a real accuracy)\n",
    "#### We can download fastext models from its website which have 1+ milions vector\n",
    "#### After download the models we have to use spacy.load(...) to load model\n",
    "- [Download model from fastext]( https://fasttext.cc/docs/en/english-vectors.html)\n",
    "- python -m spacy init-model en /tmp/en_vectors_wiki_lg --vectors-loc wiki-news-300d-1M.vec\n",
    "- nlp = spacy.load('/tmp/en_vectors_wiki_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ca6c9bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.similarity(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "716e474e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[2:5].similarity(doc[2:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "91a8ee3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nima\\AppData\\Local\\Temp\\ipykernel_7420\\3403722548.py:3: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  apple.similarity(orange)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.35815912418710183"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple = nlp(\"apple\")\n",
    "orange = nlp(\"orange\")\n",
    "apple.similarity(orange)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66afffa",
   "metadata": {},
   "source": [
    "# Another example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "55addb1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to buy this beautiful book at the end of the week.\n",
      "similarity to fruits is 0.009302210994064808 \n",
      "\n",
      "Sales of citrus have increased over the last year.\n",
      "similarity to fruits is 0.12056183815002441 \n",
      "\n",
      "How much do you know about this type of tree?\n",
      "similarity to fruits is 0.006569376215338707 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nima\\AppData\\Local\\Temp\\ipykernel_7420\\2000832602.py:5: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  print('similarity to', token.text, 'is', token.similarity(sent),'\\n')\n"
     ]
    }
   ],
   "source": [
    "token = nlp(u'fruits')[0]\n",
    "doc = nlp(u'I want to buy this beautiful book at the end of the week. Sales of citrus have increased over the last year. How much do you know about this type of tree?')\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)\n",
    "    print('similarity to', token.text, 'is', token.similarity(sent),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5482232d",
   "metadata": {},
   "source": [
    "# Ok, Give me suggests to find similarity of entities and compare them to each other in this sentences\n",
    "“Google Search, often referred to as simply Google, is the most used search engine nowadays. It handles a huge number of searches each day.”\n",
    "\n",
    "“Microsoft Windows is a family of proprietary operating systems developed and sold by Microsoft. The company also produces a wide range of other software for desktops and servers.”\n",
    "\n",
    "“Titicaca is a large, deep, mountain lake in the Andes. It is known as the highest navigable lake in the world.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d6f5d4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Google', 'Search', 'Google']\n",
      "['Microsoft', 'Windows', 'Microsoft']\n",
      "['Titicaca', 'Andes']\n"
     ]
    }
   ],
   "source": [
    "#first sample text\n",
    "doc1 = nlp(u'Google Search, often referred to as simply Google, is the most used search engine nowadays. It handles a huge number of searches each day.')\n",
    "\n",
    "#second sample text\n",
    "doc2 = nlp(u'Microsoft Windows is a family of proprietary operating systems developed and sold by Microsoft. The company also produces a wide range of other software for desktops and servers.')\n",
    "\n",
    "#third sample text\n",
    "doc3 = nlp(u'Titicaca is a large, deep, mountain lake in the Andes. It is known as the highest navigable lake in the world.')\n",
    "\n",
    "docs = [doc1,doc2,doc3]\n",
    "spans = {}\n",
    "\n",
    "for j,doc in enumerate(docs):\n",
    "    named_entity_span = [doc[i].text for i in range(len(doc)) if doc[i].ent_type != 0]\n",
    "    print(named_entity_span)\n",
    "    named_entity_span = ' '.join(named_entity_span)\n",
    "    named_entity_span = nlp(named_entity_span)\n",
    "    spans.update({j:named_entity_span})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "21f52249",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc1 is similar to doc2: 0.8342535026823839\n",
      "doc1 is similar to doc3: 0.7576086055423097\n",
      "doc2 is similar to doc3: 0.7173840097389196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nima\\AppData\\Local\\Temp\\ipykernel_7420\\369784022.py:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  print('doc1 is similar to doc2:',spans[0].similarity(spans[1]))\n",
      "C:\\Users\\Nima\\AppData\\Local\\Temp\\ipykernel_7420\\369784022.py:2: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  print('doc1 is similar to doc3:',spans[0].similarity(spans[2]))\n",
      "C:\\Users\\Nima\\AppData\\Local\\Temp\\ipykernel_7420\\369784022.py:3: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  print('doc2 is similar to doc3:',spans[1].similarity(spans[2]))\n"
     ]
    }
   ],
   "source": [
    "print('doc1 is similar to doc2:',spans[0].similarity(spans[1]))\n",
    "print('doc1 is similar to doc3:',spans[0].similarity(spans[2]))\n",
    "print('doc2 is similar to doc3:',spans[1].similarity(spans[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8bbe5c",
   "metadata": {},
   "source": [
    "# VISUALIZATIONS\n",
    "Perhaps the simplest way to discover insights in data is to represent that data graphically.allow you to immediately identify patterns within your data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fc9d1a",
   "metadata": {},
   "source": [
    "# displaCy Dependency Visualizer\n",
    "The displaCy dependency visualizer generates a syntactic dependency visualization for a submitted text. To use its interactive demo, navigate to [sidplaCy website](https://explosion.ai/demos/displacy/). Replace the sample sentence in the “Text to parse” text box with your text, and then click the search icon (magnifying glass) at the right of the box to generate a visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f025c1f",
   "metadata": {},
   "source": [
    "# Visualizing Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71832815",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c2d5686",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nima\\Projects\\nlp\\.venv\\Lib\\site-packages\\spacy\\displacy\\__init__.py:108: UserWarning: [W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n",
      "  warnings.warn(Warnings.W011)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"1737f103216e425a90dc30e046f366a0-0\" class=\"displacy\" width=\"925\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">want</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">Greek</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">pizza.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1737f103216e425a90dc30e046f366a0-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1737f103216e425a90dc30e046f366a0-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1737f103216e425a90dc30e046f366a0-0-1\" stroke-width=\"2px\" d=\"M420,264.5 C420,89.5 745.0,89.5 745.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1737f103216e425a90dc30e046f366a0-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1737f103216e425a90dc30e046f366a0-0-2\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1737f103216e425a90dc30e046f366a0-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,266.5 L587,254.5 603,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1737f103216e425a90dc30e046f366a0-0-3\" stroke-width=\"2px\" d=\"M245,264.5 C245,2.0 750.0,2.0 750.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1737f103216e425a90dc30e046f366a0-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M750.0,266.5 L758.0,254.5 742.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'dep' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n",
      "Shutting down server on port 5000.\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'I want a Greek pizza.')\n",
    "displacy.serve(doc, style='dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e042b2fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\"></br>\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Microsoft Windows\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is a family of proprietary operating systems developed and</br>sold by \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Microsoft\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ".</div>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'ent' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n",
      "Shutting down server on port 5000.\n"
     ]
    }
   ],
   "source": [
    "string = u\"\"\"\n",
    "Microsoft Windows is a family of proprietary operating systems developed and\n",
    "sold by Microsoft.\"\"\"\n",
    "\n",
    "doc = nlp(string)\n",
    "displacy.serve(doc, style='ent')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d2f3a8",
   "metadata": {},
   "source": [
    "# Text To Speech\n",
    "\n",
    "Use pyttsx3 to convert text to speech, \n",
    "Run below command to install __pyttsx3__\n",
    "\n",
    "- pip install pyttsx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64021df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3 as tts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63dadc3",
   "metadata": {},
   "source": [
    "### Initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cf9b142",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = tts.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49d44fd",
   "metadata": {},
   "source": [
    "### Get rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcb21da7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.getProperty(\"rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7633f4da",
   "metadata": {},
   "source": [
    "### Get volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afabcd4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.getProperty(\"volume\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218a6270",
   "metadata": {},
   "source": [
    "### Get voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70578684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Voice id=HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_EN-US_DAVID_11.0\n",
      "          name=Microsoft David Desktop - English (United States)\n",
      "          languages=[]\n",
      "          gender=None\n",
      "          age=None>\n",
      "------\n",
      "<Voice id=HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_EN-US_ZIRA_11.0\n",
      "          name=Microsoft Zira Desktop - English (United States)\n",
      "          languages=[]\n",
      "          gender=None\n",
      "          age=None>\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "for voice in engine.getProperty(\"voices\"):\n",
    "    print(voice)\n",
    "    print(\"------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f868d9",
   "metadata": {},
   "source": [
    "### Set property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd20f06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.setProperty('rate', 150)\n",
    "engine.setProperty('volume', 0.8)\n",
    "\n",
    "voice = engine.getProperty(\"voices\")[0].id\n",
    "engine.setProperty('voice', voice)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1330f7",
   "metadata": {},
   "source": [
    "### Now write and speek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be442a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = u\"\"\"\n",
    "Microsoft Windows is a family of proprietary operating systems developed and\n",
    "sold by Microsoft. Bill Gates announced Microsoft Windows on November 10,\n",
    "1983. Microsoft first released Windows for sale on November 20, 1985. Windows\n",
    "1.0 was initially sold for $100.00, and its sales surpassed 500,000 copies in\n",
    "April 1987. For comparison, more than a million copies of Windows 95 were sold\n",
    "in just the first 4 days.\n",
    "\"\"\"\n",
    "\n",
    "engine.say(string)\n",
    "engine.runAndWait()\n",
    "engine.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb282da",
   "metadata": {},
   "source": [
    "### Save to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63f187b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.save_to_file(string, \"test.mp3\")\n",
    "engine.runAndWait()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
